---
title: "Bayesian analysis of Neuroimaging data with R"
author: "Chris Hammill<br>Jason Lerch"
date: "2018-06-14"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(knitr)
opts_chunk$set(dpi=200, fig.height=6, fig.width=10)
#opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
```


# Outline

1. Quick overview of algorithms and datasets used today
1. Anatomy and hierarchies
1. Massively univariate classical statistics
1. Meet Reverend Thomas
1. A simple model
1. Diagnostics
1. A more complex example (if we have time)
1. ... 
1. Profit?

---

# The dataset

Describe the example we will be working with

---

# MAGeT - the algorithm

![MAGeT figure](MAGeT-fig.png)

Chakravarty MM, Steadman P, van Eede MC, Calcott RD, Gu V, Shaw P, Raznahan A, Collins DL, Lerch JP. Performing label-fusion-based segmentation using multiple automatically generated templates. Hum Brain Mapp. 2013 Oct;34(10):2635â€“54.

---

# MAGeT - the atlas

Describe atlas here

---

# Load the data

Read in the processed files

```{r}
suppressMessages(library(tidyverse))
adni <- read.csv("ADNI2_BL_MAGeT_Hippocampus_subfields.csv")
names(adni)
```


---
#Reorganize the diagnosis label

```{r}
library(forcats)
adni <- adni %>% 
  mutate(DX_bl=fct_relevel(DX_bl, 
                           "CN", "SMC", "EMCI", "LMCI", "AD"))
```

---

# Data organization

Make the dataframe long rather than wide

```{r}
adniLong <- adni %>% 
  select(-X) %>% 
    gather(structure, volume, L_CA1:R_Mam)
adniLong %>% 
  select(PTID, DX_bl, structure, volume) %>% sample_n(5)
```


---

# A quick look at the data

```{r, fig.height=6}
library(ggplot2)
ggplot(adniLong) + aes(x=DX_bl, y=AGE) + geom_boxplot()
```

---

# A quick look at hippocampal volume

```{r, tidy=FALSE, fig.height=5}
adniLong %>% group_by(PTID) %>% 
  summarize(DX=DX_bl[1], GENDER=PTGENDER[1], HPC=sum(volume)) %>%
  ggplot() + aes(DX, HPC, colour=GENDER) + geom_boxplot()
```

---

# Classic Statistics

1. Decide on model
1. Apply model to every ROI/voxel separately
1. Widen confidence intervals to account for multiple comparisons

---

# map for looping over structures

```{r}
library(broom)
adniLong %>%
  split(.$structure) %>%
  map(~lm(volume ~ AGE+PTGENDER+DX_bl, .)) %>%
  map_dfr(tidy, .id='roi') %>% head(14)
```
---

# better overview

```{r}
rTable <- adniLong %>%
  split(.$structure) %>%
  map(~lm(volume ~ AGE+PTGENDER+DX_bl, .)) %>%
  map_dfr(tidy, .id='roi') %>% 
  filter(startsWith(term, "DX")) %>%
  select(roi, term, statistic) %>%
  spread(term, statistic) 
```

---
# better overview
```{r}
rTable
```

---
# better overview
```{r}
DT::datatable(rTable %>% remove_rownames() %>% 
  column_to_rownames("roi") %>% round(2), options=list(pageLength=7))
```

---

# Frequentist Statistics

- In frequentism we frequently concern ourselves with null hypothesis testing
- Null hypotheses are reference models you compare against your data.
- If your data (and more extreme data) would be very unlikely given your null model
  you reject the null hypothesis, and conclude that the null hypothesis is not correct.
- Choosing a threshold for this probability (e.g. 0.05) and rejecting when the likelihood
  is below the threshold gives you a fixed probability of making a "Type I" error, which conveniently
  is equal to your threshold.
- So if we reject all likelihoods (p-values) when they are below 0.05 we have a 5% chance of
  rejecting when the null model is in fact true.
- If this is confusing, you're not alone, this is very hard to wrap your mind around.

---

# Dealing with Many Tests

- If you're testing a lot of hypotheses, a 5% chance of making a mistake adds up
- After 14 tests you have a better than a 50/50 chance of having made at least one mistake
- How do we control for this?
- Two main approaches Family-Wise Error Rate (FWER) control and False-Discovery Rate (FDR) control.
- In neuroimaging we tend to use FDR control.

--- 

# FDR

- Instead of trying to control our chances of making at least one mistake, let's try to control the
  fraction of mistakes we make.
- If we have 100's or more tests we can accept a few mistakes in the interest of finding the
  important results.
- To do this we employ the Benjamini-Hochberg procedure.
- The Benjamini-Hochberg procedure turns our p-values in q-values. Rejecting all q-values below some
  threshold controls the expected number of mistakes.
- For example if we reject all hypotheses with q < 0.05, we expect about 5% of our results to be
  false discoveries (type I errors). 

---

# multiple comparisons - omnibus FDR

```{r}
pTable <- adniLong %>%
  split(.$structure) %>%
  map(~lm(volume ~ AGE+PTGENDER+DX_bl, .)) %>%
  map_dfr(tidy, .id='roi') %>% 
  filter(startsWith(term, "DX")) %>%
  select(roi, term, p.value) %>%
  spread(term, p.value) %>%
  remove_rownames() %>%
  column_to_rownames("roi") %>%
  as.matrix()
qTable <- pTable
qTable[,] <- p.adjust(pTable, 'fdr')
```

---

# multiple comparisons - omnibus FDR

```{r}
qTable
```

---

# Hippocampal anatomical hierarchy

Setting up a simple hierarchy, dividing the hippocampus in grey matter and tracts.

1. Hippocampal Formation
    1. Grey Matter
        1. CA1
        1. CA2/CA3
        1. CA4/DG
        1. subiculum
        1. stratum
        1. Mammilary bodies
    1. White Matter
        1. Alveus
        1. Fimbria
        1. Fornix

---

# Hippocampal anatomical hierarchy

```{r}
library(data.tree)
hpc <- Node$new("HPC")
gm <- hpc$AddChild("GM")
ca1 <- gm$AddChild("CA1")
lca1 <- ca1$AddChild("left CA1")
lca1$volumes <- adni$L_CA1
rca1 <- ca1$AddChild("right CA1")
rca1$volumes <- adni$R_CA1
ca23 <- gm$AddChild("CA2CA3")
lca23 <- ca23$AddChild("left CA2CA3")
lca23$volumes <- adni$L_CA2CA3
rca23 <- ca23$AddChild("right CA2CA3")
rca23$volumes <- adni$R_CA2CA3
ca4 <- gm$AddChild("CA4DG")
lca4 <- ca4$AddChild("left CA4DG")
lca4$volumes <- adni$L_CA4DG
rca4 <- ca4$AddChild("right CA4DG")
rca4$volumes <- adni$R_CA4DG
subiculum <- gm$AddChild("subiculum")
lsubiculum <- subiculum$AddChild("left subiculum")
lsubiculum$volumes <- adni$L_subiculum
rsubiculum <- subiculum$AddChild("right subiculum")
rsubiculum$volumes <- adni$R_subiculum
stratum <- gm$AddChild("stratum")
lstratum <- stratum$AddChild("left stratum")
lstratum$volumes <- adni$L_stratum
rstratum <- stratum$AddChild("right stratum")
rstratum$volumes <- adni$R_stratum
mam <- gm$AddChild("Mammillary bodies")
lmam <- mam$AddChild("left Mammillary bodies")
lmam$volumes <- adni$L_Mam
rmam <- mam$AddChild("right Mammillary bodies")
rmam$volumes <- adni$R_Mam
wm <- hpc$AddChild("WM")
alveus <- wm$AddChild("Alveus")
lalveus <- alveus$AddChild("left Alveus")
lalveus$volumes <- adni$L_Alv
ralveus <- alveus$AddChild("right Alveus")
ralveus$volumes <- adni$R_Alv
fimbria <- wm$AddChild("Fimbria")
lfimbria <- fimbria$AddChild("left Fimbria")
lfimbria$volumes <- adni$L_Fimb
rfimbria <- fimbria$AddChild("right Fimbria")
rfimbria$volumes <- adni$R_Fimb
fornix <- wm$AddChild("Fornix")
lfornix <- fornix$AddChild("left Fornix")
lfornix$volumes <- adni$L_Fornix
rfornix <- fornix$AddChild("right Fornix")
rfornix$volumes <- adni$R_Fornix
```

---

# Hippocampal anatomical hierarchy

```{r}
hpc
```

---

# Hippocampal anatomical hierarchy

```{r, fig.height=6, dpi=72}
SetGraphStyle(hpc, rankdir="LR")
plot(hpc)
```

---
# Aggregate up the tree

```{r}
hpc$Do(function(x){
  x$volumes <- Aggregate(x, "volumes", rowSums)
}, traversal="post-order", filterFun=isNotLeaf)

hpc$Do(function(x) {
  x$meanVolume <- mean(x$volumes)
})
```


---

# Tree graph

```{r, fig.height=4}
library(treemap)
ToDataFrameTable(hpc, "pathString", "meanVolume", "name") %>% 
  mutate(path=strsplit(pathString, "/"), 
         struct=map_chr(path, ~ .x[3]), 
         tissue=map_chr(path, ~ .x[2])) %>% 
  select(struct, tissue, name, meanVolume) %>% 
  treemap(index=c("tissue", "struct"), vSize="meanVolume")
```


---
# Statistics on the tree

```{r}
hpc$Do(function(x){
  adni$volumes <- x$volumes
  x$stats <- 
    lm(volumes ~ AGE+PTGENDER+DX_bl, adni) %>%
    tidy() %>%
    filter(startsWith(term, "DX")) %>%
    select(term, estimate, statistic, p.value)
})
```
---

# Statistics on the tree

```{r}
print(hpc, AD=function(x) 
  x$stats %>% filter(term=="DX_blAD") %>% select(statistic) )
```

---

# Statistics on the tree

```{r}
print(hpc, AD=function(x) 
  x$stats %>% filter(term=="DX_blAD") %>% 
    select(statistic),
  filterFun=isNotLeaf )
```


---

# Statistics on the tree

```{r, fig.height=3}
hpc$Do(function(x){
  x$ad <- -log10(x$stats %>% filter(term=="DX_blAD") %>% 
                   select(p.value))
})

ToDataFrameTable(hpc, "pathString", "meanVolume", "name", "ad") %>% 
  mutate(path=strsplit(pathString, "/"), 
         struct=map_chr(path, ~ .x[3]), 
         tissue=map_chr(path, ~ .x[2])) %>% 
  select(struct, tissue, name, meanVolume, ad) %>% 
  treemap(index=c("tissue", "struct"), vSize="meanVolume", 
    vColor="ad", type="value")
```


---
class: center

# And now for Bayesianism!

---
# Why Bayesian Statistics?

Have you ever...

1. Been confused about what a p-value means?
1. Been frustrated that a difference in significance doesn't mean a significant difference?
1. Known some values for a parameter are impossible but been unable to use that to your advantage?
1. Wanted to ask more interesting questions than whether or not a parameter is or isn't zero?

---

# Why Bayesian Statistics?

Have you ever...

1. Been confused about what a p-value means?
1. Been frustrated that a difference in significance doesn't mean a significant difference?
1. Known some values for a parameter are impossible but been unable to use that to your advantage?
1. Wanted to ask more interesting questions than whether or not a parameter is or isn't zero?

Then Bayesian statistics might be right for you!

---

## How you ask?

1. De-emphasis on binary descision. Bayesians avoid null hypothesis tests, instead focusing on
estimating their parameters of interest, and reporting their uncertainty.
1. Bayesian analyses produce a distribution of possible parameter values (the posterior), that
can be used to ask many interesting questions about values. E.g. what is the probability the
effect in the hippocampus is larger than the effect in the anterior cingulate cortex.
1. Bayesian analyses can use prior information. Bayesian analysis requires an *a priori* assessment
of how likely certain parameters are. This can be vague (uninformative) or can precise (informative)
and steer your analysis away from nonsensical results.

---

class: center
# Meet The Reverend

Reverend Thomas Bayes


![](Thomas_Bayes.gif)


---
class: middle

## Bayes' Theorem

- Bayes noticed this useful property for the probabilities for two events "A" and "B"

$$ \color{red}{P(A | B)} = \frac{{\color{blue}{P(B | A)}\color{orange}{P(A)}}}{\color{magenta}{P(B)}} $$
- $\color{red}{P(A|B)}$: The probability of A given that B happened
- $\color{blue}{P(B|A)}$: The probability of B given that A happened
- $\color{orange}{P(A)}$: The probability of A
- $\color{magenta}{P(B)}$: the probability of B

- Bayes did this in the context of the binomial distribution

---
class: middle
# But Who's that behind him!

---
class: center

# It's Pierre-Simon Laplace

![](Pierre-Simon-Laplace.jpg)

---

## Bayesian Statistics

- Laplace generalized Bayes Theorem into it's modern form. While working on sex-ratios in French births.
- Start with some parameters $\theta$
- Collect some data $D$
- And deduce the probability of your parameters 

---

class: middle

# Bayes' Theorem Redux

$$ \color{red}{P(\theta | D)} = \frac{{\color{blue}{P(D | \theta)}\color{orange}{P(\theta)}}}{\color{magenta}{\int P(D | \theta)P(\theta)d\theta}} $$

**Posterior**: $\color{red}{P(\theta|D)}$: 

the probability of our parameters given our data

**Likelihood**: $\color{blue}{P(D|\theta)}$

The probability of our data given our parameters

**Prior**: $\color{orange}{P(\theta)}$

The probability of our parameters before we saw the data 

**Normalizing Constant**: $\color{magenta}{\int P(D | \theta)P(\theta)d\theta}$

The probability of the data averaged over all possible parameter sets


---

class: middle

# Bayes' Theorem Redux

$$ \color{red}{P(\theta | D)} \propto \color{blue}{P(D | \theta)}\color{orange}{P(\theta)}$$

**Posterior**: $\color{red}{P(\theta|D)}$: 

the probability of our parameters given our data

**Likelihood**: $\color{blue}{P(D|\theta)}$

The probability of our data given our parameters

**Prior**: $\color{orange}{P(\theta)}$

The probability of our parameters before we saw the data 

---

class: middle

# Bayes' Theorem Redux

$$ \color{red}{P(\theta | D)} \propto \color{orange}{P(\theta)}\color{blue}{P(D | \theta)}$$

**Posterior**: $\color{red}{P(\theta|D)}$: 

the probability of our parameters given our data

**Prior**: $\color{orange}{P(\theta)}$

The probability of our parameters before we saw the data 

**Likelihood**: $\color{blue}{P(D|\theta)}$

The probability of our data given our parameters

**Pardon the re-ordering**

---
class: middle

# Posterior

$\color{red}{P(\theta|D)}$

- The goal of bayesian statistics
- The posterior is probability distribution over parameters.
- Depends on the data we observed.
- Can be used to answer interesting questions. For
example how likely is an effect between two biologically meaninful boundaries.

---
class: middle

# Prior

$\color{orange}{P(\theta)}$

- This is what we knew before the experiment. 
- The prior is also a probability distribution over parameters.
- Doesn't depend on the data we saw.
- Gives a probability for any value the parameters could take.

---
class: middle

# Likelihood

$\color{blue}{P(D | \theta)}$

- This is how probable our data is given a hypothetical parameter set
- The likelihood is a probability distribution over data (not parameters)
- Is still a function of parameters.

---

# In words

The <font color="red">probability of parameters given our data</font> is proportional to <font color="orange">how probable we
thought they were before</font> adjusted by <font color="blue">how well they agree with the data we saw</font>.

![](posterior.gif)

---

# Many Slides of interest

---

---
# Data preparation

```{r}
suppressMessages(library(rstanarm))
hpc$Do(function(x){
  x$normVolumes <- scale(x$volumes)
})

hpcSym <- Clone(hpc)
Prune(hpcSym, isNotLeaf)
data <- hpcSym$Get("normVolumes", filterFun=isLeaf)
colnames(data) <- hpcSym$Get("name", filterFun = isLeaf)
adniLongNorm <- adni %>% 
  mutate(AGE=AGE/10) %>% 
  select(PTID:DX_bl) %>% 
  cbind(data) %>% 
  # sample_n(100) %>% # to make it not last an ice-age
  gather(structure, volume, CA1:Fornix) 
```

---
# The model

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60), eval=FALSE}
flathierarchy <- stan_lmer(
  volume ~ AGE+PTGENDER+DX_bl + (1|PTID) + (AGE+PTGENDER+DX_bl|structure), 
  adniLongNorm , chains=3, cores=3, adapt_delta = 0.99)
```

The model takes a while to run, unless you specified sample_n to a relatively small number. So just load the output from an earlier, saved run instead.

```{r}
flathierarchy <- readRDS("flathierarchy.Rds")
```

---
# Taking a peek

```{r}
summary(flathierarchy, digits=3)
```

---
# Quick diagnostics: Rhat

```{r, fig.height=4}
suppressPackageStartupMessages(library(bayesplot))
mcmc_rhat_hist(rhat(flathierarchy))
```

---
# More diagnostics: trace plots

We'll need data in matrix form for this and many future ops
```{r}
flathierarchym <- as.matrix(flathierarchy)
```

```{r, fig.height=5}
mcmc_trace(flathierarchym[,1:7])
```

---
# The main effects

```{r, fig.height=6, fig.width=10}
mcmc_intervals(flathierarchym[,1:7])
```

---
# Effect across structures
```{r, fig.height=6, fig.width=10}
mcmc_intervals(flathierarchym, 
  pars="DX_blAD", regex_pars = 'DX_blAD structure')
```

---
# Effect across structures
```{r, fig.height=6, fig.width=10}
mcmc_areas_ridges(flathierarchym, 
  pars="DX_blAD", regex_pars = 'DX_blAD structure')
```

---
# Summed effects

```{r, fig.height=5.5, fig.width=10}
vars <- colnames(flathierarchym)
ADsamples <- flathierarchym[,grep('DX_blAD structure', vars)]
mcmc_areas_ridges(ADsamples + flathierarchym[,"DX_blAD"])
```

---
# Probability calculations

What is the probability that CA1 was more affected than CA2/CA3 in AD?

```{r}
mean(flathierarchym[,'b[DX_blAD structure:CA1]'] < 
       flathierarchym[,'b[DX_blAD structure:CA2CA3]'])
```

Not very likely

```{r}
quantile(flathierarchym[,'b[DX_blAD structure:CA1]'] - 
       flathierarchym[,'b[DX_blAD structure:CA2CA3]'])
```

---
# Probability calculations
What is the probability that the subiculum was more affected than CA1 in AD?

```{r}
mean(flathierarchym[,'b[DX_blAD structure:subiculum]'] < 
       flathierarchym[,'b[DX_blAD structure:CA1]'])
```

Almost certainly

```{r}
quantile(flathierarchym[,'b[DX_blAD structure:subiculum]'] - 
       flathierarchym[,'b[DX_blAD structure:CA1]'])
```

---
# All pairs of probabilities

```{r}
ADvars <- grep("DX_blAD structure", vars)
probcomps <- matrix(nrow=length(ADvars), ncol=length(ADvars))
for (i in 1:length(ADvars)) {
  for (j in 1:length(ADvars)) {
    probcomps[i,j] <- mean(flathierarchym[,ADvars[i]] < 
                             flathierarchym[,ADvars[j]])
  }
}
```

```{r}
pnames <- sub('.+structure:(.+)\\]', '\\1', 
              colnames(flathierarchym[,ADvars]))
rownames(probcomps) <- pnames
colnames(probcomps) <- pnames
```

---
# All pairs of probabilities

```{r, fig.height=5.5}
library(pheatmap)
pheatmap(probcomps, display_numbers=T)
```

---

# Installing RMINC

- RMINC provides many convenience functions for working with hierarchically structured volume data. 
- The lead developers are responsive on github and willing to offer help if needed.
- Full install guide can be found at https://github.com/Mouse-Imaging-Centre/RMINC/blob/master/INSTALL

---

#Linux install:

1. Get the minc-toolkit-v2 (https://bic-mni.github.io/)
1. Source the configure script `source /opt/minc/1.9.16/minc-toolkit-config.sh` or where-ever your minc-toolkit version was installed
1. Set your `MINC_PATH` environment variable `export MINC_PATH=/opt/minc/1.9.16/`
1. Open an R session
1. Install `devtools` if you have not already `install.packages("devtools")`
1. Install `RMINC` with 
```{r, eval = FALSE}
devtools::install_github("Mouse-Imaging-Centre/RMINC", 
  dependencies = c("Depends", "Imports", "LinkingTo",
                   "Suggests","Enhances"))

#Mac Install

Mac install is very similar, however fortran libraries are necessary, and so you will likely need the
package manager [brew](https://brew.sh/). From there you will need to run `brew install gcc`

Then find a directory that looks like:

```{r, engine = "bash", eval = FALSE}
/usr/local/Cellar/gcc/7.*.*/lib/gcc/7
```

this needs to be added to your R build variables (makevars)

```{r, eval = FALSE, engine = "bash"}
mkdir $HOME/.R/ ## Ignore error here
touch $HOME/.R/Makevars
echo 'FLIBS=-L/usr/local/Cellar/gcc/7.1.0/lib/gcc/7' >> \
  $HOME/.R/Makevars
```

If this succeeds, go ahead and follow the linux instructions.


